# Fat-Cat: The LLM-Native Operating System

A next-generation Agent framework based on global document context and multi-stage reasoning

<p align="center">
  <img src="image.png" alt="Fat-Cat Framework" width="800">
</p>

## 1. Background & Pain Points: Why Do We Need Fat-Cat?

In the current LLM Agent development paradigm, engineers are facing two core challenges: "the quagmire of context management" and "fragile control flow". I summarize these challenges as the three original sins of Agent design:

### Pain Point One: The JSON Trap

Traditional Agent frameworks (such as early LangChain patterns or Assistant API) tend to pass state through complex JSON objects or list dictionaries.

**Problem:** LLMs are essentially trained on text. Forcing models to parse deeply nested JSON states leads to attention dilution‚Äîmodels often see the trees but miss the forest, easily overlooking critical constraints.

**Engineer's Nightmare:** When debugging, facing thousands of lines of JSON dumps makes it difficult to intuitively understand what the Agent is actually "thinking".

### Pain Point Two: The Ephemeral Knowledge Trap

While modern Agent frameworks (like LangGraph, AutoGen, CrewAI) support dynamic tool calling and multi-agent collaboration, they suffer from a critical flaw: **Knowledge Amnesia**. Each task execution is an isolated event‚Äîlessons learned, successful strategies, and domain insights evaporate once the session ends.

**The Problem:** When an Agent successfully solves a complex task (e.g., debugging a specific library error), this hard-won experience is never preserved. The next time a similar problem arises, the Agent starts from scratch, wasting tokens, time, and potentially making the same mistakes.

**What's Missing:** A persistent **Strategy Library** that accumulates problem-solving methodologies over time. Unlike humans who build expertise through experience, current Agents remain perpetual beginners‚Äîcapable but never growing wiser.

### Pain Point Three: The Black-Box Execution Crisis

Modern Agents can call tools and execute multi-step plans, but their decision-making process remains opaque. When things go wrong, engineers face an impossible debugging challenge.

**The Problem:** Current frameworks lack **Runtime Observability**. There's no structured way to understand: Why did the Agent choose this strategy? What information led to that decision? Where exactly did the reasoning break down?

**The Consequence:** When an Agent fails or produces incorrect results, you're left with cryptic logs and fragmented state. The absence of a unified, human-readable execution trace makes it nearly impossible to diagnose issues, improve prompts, or trust the Agent with critical tasks.

**What's Missing:** A **Document-Centric Audit Trail** where every stage of reasoning, strategy selection, and execution is recorded in a readable format‚Äîmaking Agent behavior transparent, debuggable, and auditable.

Fat-Cat aims to solve the above problems. It is not just a Bot that executes tasks, but an operating system prototype with "self-awareness" and "evolutionary capabilities".

## 2. Core Design Philosophy

### 2.1 LLM as Operating System

In Fat-Cat, we treat LLM as CPU, Context (document context) as memory (RAM), and external tools as peripherals (I/O).

The Fat-Cat framework itself acts as the Kernel, responsible for process scheduling (Stage switching), memory management (Memory Bridge), and exception handling (Watcher Agent).

### 2.2 Document as Global Context

We abandon fragmented JSON and adopt Markdown documents as carriers of global state. Each Stage's output is a "revision" or "supplement" to this global document.

- Stage 1 generates reasoner.md (problem analysis document)
- Stage 2 generates strategy.md (tactical manual)
- Stage 3 generates step.md (SOP execution table)
- Stage 4 executes and backfills results.

This design makes the Agent's "thinking process" completely visible and debuggable to humans.

## 3. Core Features Deep Dive: Fat-Cat's Metacognitive System

Fat-Cat's core breakthrough lies in constructing a hierarchical metacognitive closed loop. This is not simple Prompt Engineering, but rather forcing Agents to "think twice before acting" through architecture.

üß† **Stage 1: Metacognitive Analysis (Deep Intent Perception)**

"Think about how to do it before starting"

Traditional Agents receiving "help me write a crawler" might directly start writing code. But in Fat-Cat, Stage 1 Agent (Metacognitive_Analysis_agent.py) will force metacognitive analysis through reasoner.md:

- **Intent Decomposition:** Does the user really just want code, or do they need deployment?
- **Constraint Extraction:** What are the implicit language, performance, and dependency library requirements?
- **Information Completeness Check:** If information is insufficient, it will refuse to execute and request supplementation, rather than guessing blindly.

üß≠ **Stage 2: Dynamic Strategy & Metacognitive Search**

"Know what you don't know, and actively learn"

This is Fat-Cat's most innovative module (stage2_capability_upgrade_agent).

- **Strategy Retrieval:** The Agent first searches the local strategy_library for similar problem-solving experiences.
- **Metacognitive Judgment:** If the retrieved strategies have low matching scores (e.g., encountering a completely new framework or error), the Agent will trigger a **"Capability Upgrade"** signal.
- **Metacognitive Search:**

At this point, the Agent will suspend the current task and launch a subprocess to learn from the internet (via Firecrawl/Tavily). It's not searching for "answers", but rather searching for "methodologies to solve this type of problem".

**Example:** When encountering a new Python library, the Agent will first read the official documentation, summarize usage, generate a new Markdown strategy file to store in the library, and then return to solve the user's problem.

üìù **Stage 3: Logical Step Decomposition**

"Solidify thinking into instructions"

After understanding the problem (Stage 1) and learning the method (Stage 2), Stage 3 (Step_agent.py) will generate a detailed SOP (Standard Operating Procedure). This is not vague natural language, but strict steps similar to pseudocode, ensuring Stage 4's executor won't go astray.

üëÅÔ∏è **Watcher Agent: Runtime Reflection**

"An observer standing outside the system"

Watcher_Agent is an independently running daemon process. It doesn't participate in specific tasks, but monitors global document changes like watching surveillance footage.

- **Infinite Loop Detection:** If Stage 4 outputs the same error log three times consecutively.
- **Goal Deviation:** If execution results don't match the metacognitive goals defined in Stage 1.
- **Intervention Mechanism:** Watcher has the highest authority to interrupt the current Agent, force rollback, or request human intervention.

## 4. Architecture Details & Directory Structure

<p align="center">
  <img src="image.png" alt="Fat-Cat Architecture" width="800">
</p>

```bash
Fat-Cat/
‚îú‚îÄ‚îÄ agents/                 # Base Agent class definitions
‚îú‚îÄ‚îÄ ability_library/        # Core capability definitions (Markdown descriptions)
‚îú‚îÄ‚îÄ strategy_library/       # [Long-term Memory] Strategy library, storing learned problem-solving approaches
‚îú‚îÄ‚îÄ form_templates/         # Structured output templates
‚îú‚îÄ‚îÄ MCP/                    # [I/O Layer] Model Context Protocol tool implementations
‚îÇ   ‚îú‚îÄ‚îÄ code_interpreter.py # Sandboxed code interpreter
‚îÇ   ‚îú‚îÄ‚îÄ firecrawl.py        # Intelligent crawler (for metacognitive search)
‚îÇ   ‚îî‚îÄ‚îÄ tavily.py           # Search engine
‚îú‚îÄ‚îÄ Memory_system/          # [Memory Management] Handles Markdown document read/write flow
‚îú‚îÄ‚îÄ Document_Checking/      # [Memory Integrity] Prevents context loss
‚îú‚îÄ‚îÄ stage1_agent/           # [Prefrontal Cortex] Metacognitive analysis: generates reasoner.md
‚îú‚îÄ‚îÄ stage2_agent/           # [Scheduler] Strategy selection: generates strategy.md
‚îú‚îÄ‚îÄ stage2_capability_upgrade_agent/ # [Evolution Module] Responsible for metacognitive search and strategy generation
‚îú‚îÄ‚îÄ stage3_agent/           # [Commander] Step decomposition: generates step.md
‚îú‚îÄ‚îÄ stage4_agent/           # [Executor] Task execution and tool invocation
‚îú‚îÄ‚îÄ Watcher_Agent/          # [Watchdog] Runtime monitoring and exception circuit breaking
‚îú‚îÄ‚îÄ workflow/               # Pipeline orchestration
‚îú‚îÄ‚îÄ config/                 # Configuration
‚îî‚îÄ‚îÄ main.py                 # Entry point
```

## 5. Benchmark Results & Performance Evaluation

To validate the effectiveness of the Fat-Cat framework, we conducted comprehensive benchmark evaluations comparing Fat-Cat Agent against the baseline React Agent across multiple challenging tasks. The results demonstrate significant improvements in accuracy and reliability.

### Experimental Setup

We evaluated both agents on four diverse benchmark datasets, each representing different types of reasoning challenges:

- **HotPotQA (sample200)**: Multi-hop question answering requiring information synthesis across multiple documents
- **Bamboogle**: Complex web search and information retrieval tasks
- **Med_QA (‰∏≠Êñá)**: Chinese medical question answering, testing domain-specific knowledge and language understanding
- **MBPP**: Python code generation benchmark, evaluating programming capability and code correctness

Both agents were tested under identical conditions using the same LLM models and API configurations to ensure fair comparison. The LLM used was Kimi-K2.

### Performance Comparison

<p align="center">
  <img src="image2.png" alt="Performance Comparison of React Agent and Fat-Cat Agent" width="800">
</p>

### Key Findings

The benchmark results reveal consistent and substantial improvements across all evaluated tasks. As shown in the comparison chart above, Fat-Cat Agent consistently outperforms the React Agent baseline across all four benchmark datasets.

### Analysis & Insights

**1. Multi-Hop Reasoning (HotPotQA)**
The largest improvement (+12.58%) was observed in HotPotQA, which requires synthesizing information from multiple sources. Fat-Cat's metacognitive analysis (Stage 1) and strategic planning (Stage 2) enable better information gathering and cross-document reasoning compared to the reactive baseline.

**2. Code Generation (MBPP)**
Fat-Cat achieved 95.3% accuracy on MBPP, demonstrating the effectiveness of its step-by-step decomposition (Stage 3) and execution planning. The Watcher Agent's runtime monitoring helps catch errors early, preventing cascading failures.

**3. Domain-Specific Tasks (Med_QA)**
Even in specialized domains like medical QA, Fat-Cat's capability upgrade mechanism (Stage 2-C) allows it to learn domain-specific strategies, resulting in a 4% improvement over the baseline.

**4. Web Search & Retrieval (Bamboogle)**
Fat-Cat's metacognitive search capability enables more targeted information retrieval, improving accuracy by 5.4% on complex web search tasks.

### Why Fat-Cat Performs Better

The superior performance can be attributed to Fat-Cat's core architectural advantages:

1. **Metacognitive Analysis**: Stage 1's deep intent perception prevents premature execution and reduces errors from misunderstanding requirements.

2. **Dynamic Strategy Learning**: Stage 2's capability upgrade mechanism allows the Agent to learn new problem-solving approaches on-the-fly, rather than being limited to hard-coded strategies.

3. **Structured Execution**: Stage 3's logical step decomposition creates executable plans that are less prone to deviation and errors.

4. **Runtime Monitoring**: The Watcher Agent provides continuous oversight, detecting and preventing infinite loops, goal deviations, and cascading failures.

5. **Document-Centric Context**: The Markdown-based global context maintains better state coherence across complex multi-step reasoning tasks compared to fragmented JSON state management.

These results validate that Fat-Cat's metacognitive architecture and document-centric design significantly enhance Agent reliability and accuracy across diverse reasoning tasks.

## 6. Quick Start

### Requirements

- Python 3.10+
- Dependencies listed in requirements-full.txt

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/your-repo/fat-cat.git
cd fat-cat

# 2. Install dependencies (one-click script provided)
python scripts/install_full_pipeline_deps.py
```

### Configuration

Configure LLM API Key in config/model_config.py. Fat-Cat is optimized for long-context models (such as Kimi-K2). It is recommended to use models supporting 32k+ Context for the best experience.

### Running

```bash
# Start the full pipeline
python workflow/full_pipeline_runner.py
```

## 7. Developer Guide: Extension & Evolution

Fat-Cat is a living system. You can make it stronger through the following methods:

### Adding New Tools (MCP)

Inherit from _mcp_function.py in the MCP/ directory. Fat-Cat will automatically recognize and register it to Stage 4's toolbox.

### Manual Knowledge Injection (Strategy Injection)

In addition to letting the Agent learn online by itself, you can also directly add Markdown-formatted technical documents to strategy_library/. Stage 2 Agent will immediately index these new knowledge through RAG.

### Adjusting Metacognitive Thresholds

In stage2_agent, you can adjust the confidence threshold for strategy matching. The higher the threshold, the more the Agent tends to trigger "capability upgrades" to search for new knowledge rather than relying on old experience.

## 8. License

[License Information]
